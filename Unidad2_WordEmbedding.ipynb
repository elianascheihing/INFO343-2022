{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "*Este cuadernillo está basado parcialmente en material desarrollado por el Dr. Matthieu Vernier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivación: representar las dimensiones semánticas de cada palabra\n",
    "\n",
    "1. I want an orange.\n",
    "2. I want an apple.\n",
    "\n",
    "- Los enfoques <i>bag of words</i> no tienen la capacidad de calcular que las frases 1 y 2 son muy similares porque no tienen una manera de representar que las palabras 'orange' y 'apple' comparten características (<i>features</i>) comunes.\n",
    "\n",
    "- En ese contexto las palabras se representan como vectores \"1-Hot\". Por ejemplo, supongamos que tenemos un vocabulario de sólo cinco palabras: King, Queen, Man, Woman y Child. Se codificaría la palabra 'Queen' como:\n",
    "\n",
    "<img src=\"img/word2vec1.png\"/>\n",
    "\n",
    "- Sería más interesante poder representar la semántica de cada palabra tomando en cuentas ciertas características. \n",
    "\n",
    "<img src=\"img/word2vec2.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición\n",
    "\n",
    "El concepto de **word embedding** se refiere a un conjunto de técnicas utilizadas para aprender representaciones matemáticas, tipicamente vectores, de cada palabra.\n",
    "\n",
    "Una de las técnicas más populares es __Word2Vec__ propuesto por un equipo de investigación de Google en 2013 (Efficient Estimation of Word Representations in Vector Space [Mikolov et al., 2013]).\n",
    "\n",
    "Alternativas populares son __GloVe__ (propuesta por la Universidad de Stanford en 2014) y __FastText__ (propuesta por Facebook en 2016), que extende Word2Vec para considerar de mejor manera las palabras con errores ortográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas propiedades de los word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tener representaciones vectoriales de las palabras permite calcular \"razonamiento\" de tipo __King - Man + Woman = ?__ y llegar a un resultado cerca de __Queen__.\n",
    "\n",
    "<img src=\"img/word2vec4.png\"/>\n",
    "\n",
    "- Tener representaciones vectoriales de las palabras permite realizar razonamientos analógicos de tipo __A es a B, lo que C es a ..__ . Este tipo de propiedades es muy útil para aplicaciones de _Question Answering_ por ejemplo. Las respuestas a las pregutas siguientes <i>¿Cuál es la capital de Chile?</i> o <i>¿Cuáles son los clubs de fútbol en Chile?</i> se pueden responder adicionando vectores.\n",
    "\n",
    "<img src=\"img/word2vec6.png\"/>\n",
    "\n",
    "<img src=\"img/word2vec7.png\"/>\n",
    "\n",
    "<img src=\"img/word2vec8.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo se aprenden los vectores? - Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta fase de aprendizaje es aprender cuáles son las palabras _X_ más probables de aparecer en el contexto de una palabra _y_.\n",
    "\n",
    "<img src=\"img/word2vec5.png\"/>\n",
    "\n",
    "Por ejemplo, ¿cuál es la probabilidad de tener la palabra 'perro' si aparece la palabra 'pelota' en el contexto?\n",
    "\n",
    "<code>Los expertos explican que los __perros__ persiguen __pelotas__ en movimiento como parte de un comportamiento instintivo. Aunque no todos los perros tienen tan despiertos su instinto de caza, esto no impide que la mayoría de ellos sí disfruten, y mucho, de los juegos que incluyen persecuciones de una saltarina __pelota__ que bota delante de ellos. </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Los métodos de word embedding se construyen en base a grandes corpus de documentos que permiten entrenar redes neuronales artificiales con diversas arquitecturas, de tal forma que se consideran **los pesos de la capa interna de la red** como la representación de cada palabra.\n",
    "\n",
    "**Continuous bag-of-words (CBOW):** En este caso la tarea de la red neuronal es predecir una palabra dado su contexto.\n",
    "\n",
    "Para el cálculo de la verosimilitud de una palabra  $w_n$ se considera:\n",
    "\n",
    "$$w_n \\sim softmax(\\rho^T \\alpha_{d_n})$$\n",
    "\n",
    "donde $\\rho \\in M_{LxN}$ es la matriz que contiene en sus columnas el vector de embedding  de cada palabra del vocabulario y $\\alpha_{d_n} \\in R^L$ es el vector de contexto correspondiente a la suma de los vectores de embedding de las palabras en el contexto de la palabra $w_n$.\n",
    "\n",
    "**Skip-gram (SG)** La red neuronal asociada tiene como tarea predecir las palabras del contexto de una palabra dada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/modelos_embedding.png\" width=800 height=500>         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/skip-gram.png\" width=800 height=500>\n",
    "*imagen proveniente de Tesis de Magíster de Pablo Badilla, U. de Chile*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones objetivos mas utilizadas para optimizar estas redes neuronales artificiales son:\n",
    "\n",
    "- Softmax jerárquico \n",
    "\n",
    "- Negative sampling (ver https://arxiv.org/pdf/1402.3722.pdf)\n",
    "\n",
    "La biblioteca **Word2Vec** implementa CBOW y SG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para profundizar:** https://arxiv.org/pdf/1301.3781.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras arquitecturas de red y funciones objetivo definen otros métodos de word embedding:\n",
    "\n",
    "- Glove (global vectors): considera el contexto global de cada palabra en la función de optimización, no sólo el contexto local\n",
    "\n",
    "\n",
    "- Lexvec (Lexical vectors): incorpora la medida PPMI (positive point-wise mutual information) en la optimización de la arquitectura skip-gram.\n",
    "\n",
    "\n",
    "- FastText: incorpora los n-gramas que componen una palabra.\n",
    "\n",
    "\n",
    "- ConceptNet: considera la elaboración de un grafo de conocimiento (en base a diversas fuentes) con las palabras del vocabulario que se utiliza para el ajuste de la red neuronal.\n",
    "\n",
    "\n",
    "**Limitaciones de modelos de word embedding**:\n",
    "\n",
    "- Las redes pre-entrenadas para generar la representación vectorial dependen fuertemente de los corpus utilizados.\n",
    "\n",
    "- Mayormente desarrollados en inglés. Algunas iniciativas en español.### Limitaciones de los word embeddings\n",
    "\n",
    "- No permiten tomar en cuenta el orden entre las palabras.\n",
    "\n",
    "Ejemplo: \"Estamos aqui para trabajar y no jugar\" vs. \"Estamos aqui para jugar y no trabajar\"\n",
    "\n",
    "- No permiten tomar en cuenta que ciertas palabras cambian de significado según el contexto.\n",
    "\n",
    "Ejemplo: \"I lost my computer __mouse__\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para mejorar algunas de estas limitaciones:**\n",
    "\n",
    "- Combinar Word Embedding con redes neuronales (convolucionales (CNN) o secuenciales (Transformers, BERT, GPT-3)) que toman en cuenta el orden entre las palabras y el contexto de las palabras.\n",
    "\n",
    "- **Attention Word Embedding (AWE)** Incorpora mejoras en el proceso de optimización en CBOW de manera que el vector de contexto que se utiliza para definir la función objetivo no le de la misma importancia a todas las palabras del contexto, ajustando pesos distintos de las palabras en el contexto de la palabra a predecir mediante mecanismos de atención (matrices key y query). Existe también una versión AWE-S que considera cada palabra descompuesta en n-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Topic Models (ETM)\n",
    "\n",
    "(ver https://arxiv.org/pdf/1907.04907.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se propone el siguiente proceso generativo secuencial:\n",
    "\n",
    "\n",
    "1. Por cada documento generar la distribución de tópicos de una **Logistic Normal**:\n",
    "\n",
    "$$\\theta_d \\sim {\\cal{LN}}(0,I_{KxK})$$\n",
    "    \n",
    "2. Por cada documento $d$ generar la palabra $n$ de la manera siguiente:\n",
    "\n",
    "    a. Asignar un tópico:\n",
    "    \n",
    "    $$z_{d,n} \\sim Mult(\\theta_d)$$\n",
    "    \n",
    "    b. Generar la palabra:\n",
    "    \n",
    "    $$w_{d,n} \\sim Mult(softmax(\\rho^T \\alpha_{z_{d,n}}))$$\n",
    "    \n",
    "donde $\\rho \\in M_{LxN}$ es la matriz que contiene en sus columnas el vector de embedding  de cada palabra del vocabulario y $\\alpha_{z_{d_n}} \\in R^L$ es el vector de embedding del tópico $z_{d,n}$.\n",
    "        \n",
    "<img src=\"img/ETM.png\" width=600 height=200>            \n",
    "\n",
    "Como en los modelos anteriores, las distribuciones a posteriori no tienen expresiones analíticas, por lo que se recurre a la Inferencia Variacional para aproximarlas.\n",
    "\n",
    "Los resultados de ETM  mejoran sustancialmente el desempeño de LDA al aumentar el tamaño del vocabulario.\n",
    "\n",
    "\n",
    "<img src=\"img/etm_vs-lda.png\" width=400 height=400>       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
